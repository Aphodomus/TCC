{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Embeddings Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import emoji\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load('pt_core_news_md')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [\n",
    "    'SUBJECT',\n",
    "    'SENDER',\n",
    "    'EMAIL',\n",
    "    'DELIVERED',\n",
    "    'HOUR_OF_DAY',\n",
    "    'WEEKDAY',\n",
    "    'TIME_OF_DAY',\n",
    "    'DELIVERED_SCALED',\n",
    "    'OPEN_RATE',\n",
    "    'HOUR_OPEN_RATE',\n",
    "    'WEEKDAY_OPEN_RATE',\n",
    "    'TIME_OF_DAY_OPEN_RATE',\n",
    "    'SENDER_OPEN_RATE',\n",
    "    'EMAIL_OPEN_RATE',\n",
    "    'COMBINED_TEXT',\n",
    "    'SUBJECT_PREPROCESSED',\n",
    "    'EMBEDDING_TFIDF',\n",
    "    'EMBEDDING_WORD2VEC',\n",
    "    'EMBEDDING_OPENAI'\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    'SUBJECT': 'string',\n",
    "    'SENDER': 'category',\n",
    "    'EMAIL': 'category',\n",
    "    'DELIVERED': 'int64',\n",
    "    'OPEN_RATE': 'float64',\n",
    "    'HOUR_OF_DAY': 'int64',\n",
    "    'WEEKDAY': 'category',\n",
    "    'TIME_OF_DAY': 'category',\n",
    "    'DELIVERED_SCALED': 'float64',\n",
    "    'OPEN_RATE': 'float64',\n",
    "    'HOUR_OPEN_RATE': 'float64',\n",
    "    'WEEKDAY_OPEN_RATE': 'float64',\n",
    "    'TIME_OF_DAY_OPEN_RATE': 'float64',\n",
    "    'SENDER_OPEN_RATE': 'float64',\n",
    "    'EMAIL_OPEN_RATE': 'float64',\n",
    "    'COMBINED_TEXT': 'string',\n",
    "    'SUBJECT_PREPROCESSED': 'string',\n",
    "    'EMBEDDING_TFIDF': 'string',\n",
    "    'EMBEDDING_WORD2VEC': 'string',\n",
    "    'EMBEDDING_OPENAI': 'string'\n",
    "}\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    train = pd.read_csv('./data/train.csv', dtype=dtypes)[all_columns]\n",
    "    test = pd.read_csv('./data/test.csv', dtype=dtypes)[all_columns]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def contains_emoji(text):\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def extract_features_morphologic(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    columns_to_drop = all_columns.copy()\n",
    "    columns_to_drop.remove('OPEN_RATE')\n",
    "    \n",
    "    new_df['SUBJECT_LENGTH'] = new_df['SUBJECT'].apply(len)\n",
    "    new_df['SUBJECT_WORD_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(x.split()))\n",
    "    new_df['SUBJECT_SPECIAL_CHARS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "    new_df['SUBJECT_NUMBERS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    new_df['SUBJECT_HAS_EMOJI'] = new_df['SUBJECT'].apply(contains_emoji)\n",
    "    \n",
    "    new_df['SUBJECT_UPPERCASE_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "    new_df['SUBJECT_NAMED_ENTITIES_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(nlp(x).ents))\n",
    "\n",
    "    new_df['SUBJECT_LETTER_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(c.isalpha() for c in x) / len(x))\n",
    "    new_df['SUBJECT_VOWEL_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'aeiouáéíóúâêîôûãõ' for c in x))\n",
    "    new_df['SUBJECT_CONSONANT_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'bcçdfghjklmnpqrstvwxyz' for c in x))\n",
    "    new_df['SUBJECT_PUNCTUATION_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(el in string.punctuation for el in x))\n",
    "    \n",
    "    return new_df.drop(columns=columns_to_drop)\n",
    "\n",
    "def extract_features_subject(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'OPEN_RATE',\n",
    "        'DELIVERED',\n",
    "        'DELIVERED_SCALED',\n",
    "        'HOUR_OF_DAY',\n",
    "        'WEEKDAY',\n",
    "        'TIME_OF_DAY',\n",
    "        'HOUR_OPEN_RATE',\n",
    "        'WEEKDAY_OPEN_RATE',\n",
    "        'TIME_OF_DAY_OPEN_RATE',\n",
    "        'SENDER_OPEN_RATE',\n",
    "        'EMAIL_OPEN_RATE',\n",
    "        'EMAIL',\n",
    "        'SUBJECT',\n",
    "        'EMBEDDING_TFIDF',\n",
    "        'EMBEDDING_WORD2VEC',\n",
    "        'EMBEDDING_OPENAI'\n",
    "    ]\n",
    "    \n",
    "    columns_to_drop = all_columns.copy()\n",
    "    for col in columns_to_keep:\n",
    "        columns_to_drop.remove(col)\n",
    "    \n",
    "    new_df['SUBJECT_LENGTH'] = new_df['SUBJECT'].apply(len)\n",
    "    new_df['SUBJECT_WORD_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(x.split()))\n",
    "    new_df['SUBJECT_SPECIAL_CHARS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "    new_df['SUBJECT_NUMBERS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    new_df['SUBJECT_HAS_EMOJI'] = new_df['SUBJECT'].apply(contains_emoji)\n",
    "    \n",
    "    new_df['SUBJECT_UPPERCASE_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "    new_df['SUBJECT_NAMED_ENTITIES_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(nlp(x).ents))\n",
    "\n",
    "    new_df['SUBJECT_LETTER_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(c.isalpha() for c in x) / len(x))\n",
    "    new_df['SUBJECT_VOWEL_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'aeiouáéíóúâêîôûãõ' for c in x))\n",
    "    new_df['SUBJECT_CONSONANT_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'bcçdfghjklmnpqrstvwxyz' for c in x))\n",
    "    new_df['SUBJECT_PUNCTUATION_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(el in string.punctuation for el in x))\n",
    "    \n",
    "    return new_df.drop(columns=columns_to_drop)\n",
    "\n",
    "def remove_acentos(text):\n",
    "    normalized_text = unicodedata.normalize('NFD', text)\n",
    "    text_without_accents = ''.join(c for c in normalized_text if unicodedata.category(c) != 'Mn')\n",
    "    return text_without_accents\n",
    "\n",
    "def remove_numeros(sentence):\n",
    "    return ' '.join('<NUM>' if token.isdigit() else token for token in sentence.split())\n",
    "\n",
    "def preprocessing_text(text, language='portuguese'):\n",
    "    text = remove_acentos(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('r$', '')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = remove_numeros(text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized_tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocessing_subject(data):\n",
    "    feature_selected = extract_features_subject(data)\n",
    "    \n",
    "    return feature_selected\n",
    "\n",
    "def encoding(train, test):\n",
    "    categorical_columns = train.select_dtypes(include=['category']).columns.tolist()  # Obtém todas as colunas categóricas\n",
    "    \n",
    "    encoded_train = train.copy()\n",
    "    encoded_test = test.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        encoder.fit(train[[col]])\n",
    "        \n",
    "        train_encoded = encoder.transform(train[[col]]).toarray()\n",
    "        test_encoded = encoder.transform(test[[col]]).toarray()\n",
    "        \n",
    "        columns = encoder.get_feature_names_out([col])\n",
    "        \n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=columns, index=train.index)\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=columns, index=test.index)\n",
    "        \n",
    "        encoded_train = pd.concat([encoded_train.reset_index(drop=True), train_encoded_df.reset_index(drop=True)], axis=1)\n",
    "        encoded_test = pd.concat([encoded_test.reset_index(drop=True), test_encoded_df.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        encoded_train = encoded_train.drop(col, axis=1)\n",
    "        encoded_test = encoded_test.drop(col, axis=1)\n",
    "    \n",
    "    return encoded_train, encoded_test\n",
    "\n",
    "def normalize(train, test):\n",
    "    num_columns = train.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    columns_to_remove = ['OPEN_RATE'] + [col for col in num_columns if 'EMBEDDING' in col]\n",
    "    num_columns = num_columns.drop(columns_to_remove)\n",
    "    \n",
    "    X_train_num = train[num_columns]\n",
    "    X_test_num = test[num_columns]\n",
    "\n",
    "    train_non_num = train.drop(columns=num_columns)\n",
    "    test_non_num = test.drop(columns=num_columns)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_num)\n",
    "\n",
    "    X_train_normalized = scaler.transform(X_train_num)\n",
    "    X_test_normalized = scaler.transform(X_test_num)\n",
    "\n",
    "    train_normalized_num = pd.DataFrame(X_train_normalized, columns=num_columns, index=train.index)\n",
    "    test_normalized_num = pd.DataFrame(X_test_normalized, columns=num_columns, index=test.index)\n",
    "\n",
    "    train_final = pd.concat([train_normalized_num, train_non_num], axis=1)\n",
    "    test_final = pd.concat([test_normalized_num, test_non_num], axis=1)\n",
    "\n",
    "    return train_final, test_final\n",
    "\n",
    "def load_model(name, version):\n",
    "    models = {\n",
    "        'support_vector_regressor': SVR,\n",
    "        'decision_tree_regressor': DecisionTreeRegressor,\n",
    "        'random_forest_regressor': RandomForestRegressor,\n",
    "        'gradient_boosting_regressor': GradientBoostingRegressor,\n",
    "        'xgboost_regressor': XGBRegressor,\n",
    "        'lightgbm_regressor': LGBMRegressor,\n",
    "        'catboost_regressor': CatBoostRegressor,\n",
    "        'multilayer_perceptron_regressor': MLPRegressor,\n",
    "    }\n",
    "    \n",
    "    if name not in models:\n",
    "        raise ValueError(\"Model name not supported\")\n",
    "    \n",
    "    model = joblib.load(f'./models/{name}_{version}_model.pkl')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, r2, mape\n",
    "\n",
    "def create_transform_target_regressor(model):\n",
    "    return TransformedTargetRegressor(\n",
    "        regressor=model,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    )\n",
    "\n",
    "def train_model(model, model_name, version, X_train, y_train, X_test, y_test):\n",
    "    model_regressor = create_transform_target_regressor(model)\n",
    "    \n",
    "    model_regressor.fit(\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "    \n",
    "    joblib.dump(model_regressor, f'./models/{model_name}_{version}_model.pkl')\n",
    "    model_regressor = load_model(model_name, version)\n",
    "    \n",
    "    y_pred = model_regressor.predict(X_test)\n",
    "    \n",
    "    rmse, mae, r2, mape = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'version': version,\n",
    "        'rmse': round(rmse, 4),\n",
    "        'mae': round(mae, 4),\n",
    "        'r²': round(r2, 4),\n",
    "        'mape': round(mape, 4)\n",
    "    }\n",
    "\n",
    "def models(version, X_train, y_train, X_test, y_test):\n",
    "    model_dict = {\n",
    "        'support_vector_regressor': SVR(),\n",
    "        'decision_tree_regressor': DecisionTreeRegressor(random_state=42, max_depth=2),\n",
    "        'random_forest_regressor': RandomForestRegressor(random_state=42),\n",
    "        'gradient_boosting_regressor': GradientBoostingRegressor(random_state=42),\n",
    "        'lightgbm_regressor': LGBMRegressor(random_state=42, verbose=-1, force_row_wise=True),\n",
    "        'xgboost_regressor': XGBRegressor(seed=42),\n",
    "        'catboost_regressor': CatBoostRegressor(random_seed=42, silent=True),\n",
    "        'multilayer_perceptron_regressor': MLPRegressor(\n",
    "            hidden_layer_sizes=(1024,),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            batch_size='auto',\n",
    "            learning_rate='constant',\n",
    "            learning_rate_init=0.001,\n",
    "            max_iter=200,\n",
    "            tol=0.0001,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    print(f'\\nVersion - {version}')\n",
    "    results = []\n",
    "    for model_name, model in model_dict.items():\n",
    "        result = train_model(model, model_name, version, X_train, y_train, X_test, y_test)\n",
    "        results.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def models_final(version, X_train, y_train, X_test, y_test):\n",
    "    model_dict = {\n",
    "        'catboost_regressor': CatBoostRegressor(random_seed=42, silent=True),\n",
    "    }\n",
    "    print(f'\\nVersion - {version}')\n",
    "    results = []\n",
    "    for model_name, model in model_dict.items():\n",
    "        result = train_model(model, model_name, version, X_train, y_train, X_test, y_test)\n",
    "        results.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_with_embeddings(train, test, embedding_type):\n",
    "    train_embeddings = pd.DataFrame(train[embedding_type].tolist())\n",
    "    test_embeddings = pd.DataFrame(test[embedding_type].tolist())\n",
    "    \n",
    "    X_train = train_embeddings\n",
    "    y_train = train['OPEN_RATE']\n",
    "    X_test = test_embeddings\n",
    "    y_test = test['OPEN_RATE']\n",
    "\n",
    "    return models(embedding_type.lower(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "def expand_embeddings(data, column_name):\n",
    "    if column_name in data.columns:\n",
    "        embeddings_df = pd.DataFrame(data[column_name].tolist())\n",
    "        embeddings_df.columns = [f'{column_name}_{i}' for i in range(embeddings_df.shape[1])]\n",
    "        return embeddings_df\n",
    "    else:\n",
    "        print(f\"A coluna {column_name} não foi encontrada.\")\n",
    "    return data\n",
    "\n",
    "def train_with_features_embeddings(train, test, embedding_type):\n",
    "    column_name = f'{embedding_type.upper()}'\n",
    "    train_embeddings = expand_embeddings(train, column_name)\n",
    "    test_embeddings = expand_embeddings(test, column_name)\n",
    "    \n",
    "    train_normalized_embeddings = pd.concat([train.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), train_embeddings], axis=1)\n",
    "    test_normalized_embeddings = pd.concat([test.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), test_embeddings], axis=1)\n",
    "    \n",
    "    X_train = train_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_train = train_normalized_embeddings['OPEN_RATE']\n",
    "    X_test = test_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_test = test_normalized_embeddings['OPEN_RATE']\n",
    "\n",
    "    embedding_type = 'OPERATIONAL_' + embedding_type\n",
    "    return models(embedding_type.lower(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_with_features_embeddings_pca(train, test, embedding_type):\n",
    "    column_name = f'{embedding_type.upper()}'\n",
    "    train_embeddings = expand_embeddings(train, column_name)\n",
    "    test_embeddings = expand_embeddings(test, column_name)\n",
    "    \n",
    "    pca = PCA(n_components=6, random_state=42)\n",
    "    train_vecs_df = pca.fit_transform(train_embeddings)\n",
    "    test_vecs_df = pca.transform(test_embeddings)\n",
    "    \n",
    "    pca_column_names = [\"w2v_pca_\" + str(i) for i in range(train_vecs_df.shape[1])]\n",
    "    train_vecs_df = pd.DataFrame(train_vecs_df, columns=pca_column_names, index=train.index)\n",
    "    test_vecs_df = pd.DataFrame(test_vecs_df, columns=pca_column_names, index=test.index)\n",
    "    \n",
    "    train_normalized_embeddings = pd.concat([train.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), train_vecs_df], axis=1)\n",
    "    test_normalized_embeddings = pd.concat([test.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), test_vecs_df], axis=1)\n",
    "    \n",
    "    X_train = train_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_train = train_normalized_embeddings['OPEN_RATE']\n",
    "    X_test = test_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_test = test_normalized_embeddings['OPEN_RATE']\n",
    "\n",
    "    embedding_type = 'OPERATIONAL_' + embedding_type + '_pca'\n",
    "    return models(embedding_type.lower(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_final(train, test, embedding_type):\n",
    "    column_name = f'{embedding_type.upper()}'\n",
    "    train_embeddings = expand_embeddings(train, column_name)\n",
    "    test_embeddings = expand_embeddings(test, column_name)\n",
    "    \n",
    "    pca = PCA(n_components=6, random_state=42)\n",
    "    train_vecs_df = pca.fit_transform(train_embeddings)\n",
    "    test_vecs_df = pca.transform(test_embeddings)\n",
    "    \n",
    "    pca_column_names = [\"w2v_pca_\" + str(i) for i in range(train_vecs_df.shape[1])]\n",
    "    train_vecs_df = pd.DataFrame(train_vecs_df, columns=pca_column_names, index=train.index)\n",
    "    test_vecs_df = pd.DataFrame(test_vecs_df, columns=pca_column_names, index=test.index)\n",
    "    \n",
    "    train_normalized_embeddings = pd.concat([train.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), train_vecs_df], axis=1)\n",
    "    test_normalized_embeddings = pd.concat([test.drop(columns=['EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC', 'EMBEDDING_OPENAI']), test_vecs_df], axis=1)\n",
    "    \n",
    "    X_train = train_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_train = train_normalized_embeddings['OPEN_RATE']\n",
    "    X_test = test_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "    y_test = test_normalized_embeddings['OPEN_RATE']\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    embedding_type = 'OPERATIONAL_' + embedding_type + '_pca'\n",
    "    return models_final(embedding_type.lower(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "def save_results(results_list, path):\n",
    "    data = []\n",
    "\n",
    "    for results, version in results_list:\n",
    "        for result in results:\n",
    "            data.append({\n",
    "                'model_name': result['model_name'],\n",
    "                'version': version,\n",
    "                'rmse': result['rmse'],\n",
    "                'mae': result['mae'],\n",
    "                'r²': result['r²'],\n",
    "                'mape': result['mape']\n",
    "            })\n",
    "        \n",
    "    with open(path, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile)\n",
    "        \n",
    "def extract_features_operational(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'OPEN_RATE',\n",
    "        'DELIVERED',\n",
    "        'DELIVERED_SCALED',\n",
    "        'HOUR_OF_DAY',\n",
    "        'WEEKDAY',\n",
    "        'TIME_OF_DAY',\n",
    "        'HOUR_OPEN_RATE',\n",
    "        'WEEKDAY_OPEN_RATE',\n",
    "        'TIME_OF_DAY_OPEN_RATE',\n",
    "        'SENDER_OPEN_RATE',\n",
    "        'EMAIL_OPEN_RATE',\n",
    "        'EMAIL'\n",
    "    ]\n",
    "    \n",
    "    columns_to_drop = all_columns.copy()\n",
    "    for col in columns_to_keep:\n",
    "        columns_to_drop.remove(col)\n",
    "    \n",
    "    new_df['SUBJECT_LENGTH'] = new_df['SUBJECT'].apply(len)\n",
    "    new_df['SUBJECT_WORD_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(x.split()))\n",
    "    new_df['SUBJECT_SPECIAL_CHARS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "    new_df['SUBJECT_NUMBERS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    new_df['SUBJECT_HAS_EMOJI'] = new_df['SUBJECT'].apply(contains_emoji)\n",
    "    \n",
    "    new_df['SUBJECT_UPPERCASE_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "    new_df['SUBJECT_NAMED_ENTITIES_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(nlp(x).ents))\n",
    "\n",
    "    new_df['SUBJECT_LETTER_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(c.isalpha() for c in x) / len(x))\n",
    "    new_df['SUBJECT_VOWEL_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'aeiouáéíóúâêîôûãõ' for c in x))\n",
    "    new_df['SUBJECT_CONSONANT_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'bcçdfghjklmnpqrstvwxyz' for c in x))\n",
    "    new_df['SUBJECT_PUNCTUATION_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(el in string.punctuation for el in x))\n",
    "    \n",
    "    return new_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "def select_extraction(data, version):\n",
    "    if version == 'morphologic':\n",
    "        return extract_features_morphologic(data)\n",
    "    if version == 'operational':\n",
    "        return extract_features_operational(data)\n",
    "\n",
    "def train_with_features(train, test, version):\n",
    "    train_features = select_extraction(train, version)\n",
    "    test_features = select_extraction(test, version)\n",
    "\n",
    "    train_encoded, test_encoded = encoding(train_features, test_features)\n",
    "    train_normalized, test_normalized = normalize(train_encoded, test_encoded)\n",
    "\n",
    "    X_train = train_normalized.drop('OPEN_RATE', axis=1)\n",
    "    y_train = train_normalized['OPEN_RATE']\n",
    "    X_test = test_normalized.drop('OPEN_RATE', axis=1)\n",
    "    y_test = test_normalized['OPEN_RATE']\n",
    "\n",
    "    return models(version.lower(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_and_prepare_data()\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = preprocessing_subject(train)\n",
    "test_features = preprocessing_subject(test)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded = encoding(train_features, test_features)\n",
    "train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normalized, test_normalized = normalize(train_encoded, test_encoded)\n",
    "train_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dropped = train_normalized.drop(columns=['SUBJECT'])\n",
    "test_dropped = test_normalized.drop(columns=['SUBJECT'])\n",
    "train_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform String to Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dropped.EMBEDDING_TFIDF = train_normalized.EMBEDDING_TFIDF.apply(eval).apply(np.array)\n",
    "train_dropped.EMBEDDING_WORD2VEC = train_normalized.EMBEDDING_WORD2VEC.apply(eval).apply(np.array)\n",
    "train_dropped.EMBEDDING_OPENAI = train_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)\n",
    "\n",
    "test_dropped.EMBEDDING_TFIDF = test_normalized.EMBEDDING_TFIDF.apply(eval).apply(np.array)\n",
    "test_dropped.EMBEDDING_WORD2VEC = test_normalized.EMBEDDING_WORD2VEC.apply(eval).apply(np.array)\n",
    "test_dropped.EMBEDDING_OPENAI = test_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Only Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_only_tfidf = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_TFIDF')\n",
    "result_only_word2vec = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_WORD2VEC')\n",
    "result_only_openai = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_OPENAI')\n",
    "\n",
    "results_list = [\n",
    "    (result_only_tfidf, 'tfidf'),\n",
    "    (result_only_word2vec, 'word2vec'),\n",
    "    (result_only_openai, 'openai'),\n",
    "]\n",
    "\n",
    "save_results(results_list, './data/results_only_embeddings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    'xgboost_regressor': 'XGBoost',\n",
    "    'random_forest_regressor': 'Random Forest',\n",
    "    'support_vector_regressor': 'Support Vector',\n",
    "    'multilayer_perceptron_regressor': 'Multilayer Perceptron',\n",
    "    'decision_tree_regressor': 'Decision Tree',\n",
    "    'gradient_boosting_regressor': 'Gradient Boosting',\n",
    "    'lightgbm_regressor': 'LightGBM',\n",
    "    'catboost_regressor': 'CatBoost'\n",
    "}\n",
    "\n",
    "version_mapping = {\n",
    "    'tfidf': 'Embedding TF-IDF',\n",
    "    'word2vec': 'Embedding Word2Vec',\n",
    "    'openai': 'Embedding OpenAI'\n",
    "}\n",
    "\n",
    "with open('./data/results_only_embeddings.json') as jsonfile:\n",
    "    result_only_embeddings = json.load(jsonfile)\n",
    "    \n",
    "result_only_embeddings = pd.DataFrame(result_only_embeddings)\n",
    "\n",
    "result_only_embeddings['model_name'] = result_only_embeddings['model_name'].map(model_name_mapping)\n",
    "result_only_embeddings['version'] = result_only_embeddings['version'].map(version_mapping)\n",
    "\n",
    "def plot_grouped_bars(result_only_embeddings, metrics):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    bar_width = 0.4\n",
    "    spacing = 0.3\n",
    "    models = result_only_embeddings['model_name'].unique()\n",
    "    versions = result_only_embeddings['version'].unique()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        bars = {}\n",
    "        for version in versions:\n",
    "            bars[version] = result_only_embeddings[result_only_embeddings['version'] == version].set_index('model_name')[metric].reindex(models)\n",
    "\n",
    "        x = np.arange(len(models)) * (len(versions) * bar_width + spacing)\n",
    "        for i, version in enumerate(versions):\n",
    "            axs[idx].bar(x + i * bar_width, bars[version], bar_width, label=version)\n",
    "        \n",
    "        axs[idx].set_title(f'{metric.upper()} Comparison')\n",
    "        axs[idx].set_xlabel(None)\n",
    "        axs[idx].set_ylabel(metric.upper())\n",
    "        axs[idx].set_xticks(x + (len(versions) * bar_width - bar_width) / 2)\n",
    "        axs[idx].set_xticklabels(models, rotation=35, ha='right')\n",
    "        axs[idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(versions), bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig('./images/comparison_plot_only_embeddings.png', facecolor='white', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "metrics = ['rmse', 'mae', 'mape', 'r²']\n",
    "plot_grouped_bars(result_only_embeddings, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Only Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_only_morphologic = train_with_features(train, test, 'morphologic')\n",
    "result_only_operational = train_with_features(train, test, 'operational')\n",
    "\n",
    "results_list = [\n",
    "    (result_only_morphologic, 'morphologic'),\n",
    "    (result_only_operational, 'operational'),\n",
    "]\n",
    "\n",
    "save_results(results_list, './data/results_only_features.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    'xgboost_regressor': 'XGBoost',\n",
    "    'random_forest_regressor': 'Random Forest',\n",
    "    'support_vector_regressor': 'Support Vector',\n",
    "    'multilayer_perceptron_regressor': 'Multilayer Perceptron',\n",
    "    'decision_tree_regressor': 'Decision Tree',\n",
    "    'gradient_boosting_regressor': 'Gradient Boosting',\n",
    "    'lightgbm_regressor': 'LightGBM',\n",
    "    'catboost_regressor': 'CatBoost'\n",
    "}\n",
    "\n",
    "version_mapping = {\n",
    "    'morphologic': 'Morphologic',\n",
    "    'operational': 'Operational'\n",
    "}\n",
    "\n",
    "with open('./data/results_only_features.json') as jsonfile:\n",
    "    result_only_features = json.load(jsonfile)\n",
    "    \n",
    "result_only_features = pd.DataFrame(result_only_features)\n",
    "\n",
    "result_only_features['model_name'] = result_only_features['model_name'].map(model_name_mapping)\n",
    "result_only_features['version'] = result_only_features['version'].map(version_mapping)\n",
    "\n",
    "def plot_grouped_bars(result_only_features, metrics):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    bar_width = 0.4\n",
    "    spacing = 0.3\n",
    "    models = result_only_features['model_name'].unique()\n",
    "    versions = result_only_features['version'].unique()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        bars = {}\n",
    "        for version in versions:\n",
    "            bars[version] = result_only_features[result_only_features['version'] == version].set_index('model_name')[metric].reindex(models)\n",
    "\n",
    "        x = np.arange(len(models)) * (len(versions) * bar_width + spacing)\n",
    "        for i, version in enumerate(versions):\n",
    "            axs[idx].bar(x + i * bar_width, bars[version], bar_width, label=version)\n",
    "        \n",
    "        axs[idx].set_title(f'{metric.upper()} Comparison')\n",
    "        axs[idx].set_xlabel(None)\n",
    "        axs[idx].set_ylabel(metric.upper())\n",
    "        axs[idx].set_xticks(x + (len(versions) * bar_width - bar_width) / 2)\n",
    "        axs[idx].set_xticklabels(models, rotation=35, ha='right')\n",
    "        axs[idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(versions), bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig('./images/comparison_plot_only_features.png', facecolor='white', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "metrics = ['rmse', 'mae', 'mape', 'r²']\n",
    "plot_grouped_bars(result_only_features, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Features + Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_morphologic = train_with_features(train, test, 'morphologic')\n",
    "result_operational = train_with_features(train, test, 'operational')\n",
    "result_only_tfidf = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_TFIDF')\n",
    "result_only_word2vec = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_WORD2VEC')\n",
    "result_only_openai = train_with_embeddings(train_dropped, test_dropped, 'EMBEDDING_OPENAI')\n",
    "result_operational_tfidf_pca = train_with_features_embeddings_pca(train_dropped, test_dropped, 'EMBEDDING_TFIDF')\n",
    "result_operational_word2vec_pca = train_with_features_embeddings_pca(train_dropped, test_dropped, 'EMBEDDING_WORD2VEC')\n",
    "result_operational_openai_pca = train_with_features_embeddings_pca(train_dropped, test_dropped, 'EMBEDDING_OPENAI')\n",
    "\n",
    "results_list = [\n",
    "    (result_morphologic, 'morphologic'),\n",
    "    (result_operational, 'operational'),\n",
    "    (result_only_tfidf, 'tfidf'),\n",
    "    (result_only_word2vec, 'word2vec'),\n",
    "    (result_only_openai, 'openai'),\n",
    "    (result_operational_tfidf_pca, 'operational+tfidf+pca'),\n",
    "    (result_operational_word2vec_pca, 'operational+word2vec+pca'),\n",
    "    (result_operational_openai_pca, 'operational+openai+pca'),\n",
    "]\n",
    "\n",
    "save_results(results_list, './data/results_final.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    'xgboost_regressor': 'XGBoost',\n",
    "    'random_forest_regressor': 'Random Forest',\n",
    "    'support_vector_regressor': 'Support Vector',\n",
    "    'multilayer_perceptron_regressor': 'Multilayer Perceptron',\n",
    "    'decision_tree_regressor': 'Decision Tree',\n",
    "    'gradient_boosting_regressor': 'Gradient Boosting',\n",
    "    'lightgbm_regressor': 'LightGBM',\n",
    "    'catboost_regressor': 'CatBoost'\n",
    "}\n",
    "\n",
    "version_mapping = {\n",
    "    'morphologic': 'Morphologic',\n",
    "    'operational': 'Operational',\n",
    "    'openai': 'Embedding OpenAI',\n",
    "    'tfidf': 'Embedding TF-IDF',\n",
    "    'word2vec': 'Embedding Word2Vec',\n",
    "    'openai': 'Embedding OpenAI',\n",
    "    'operational+tfidf+pca': 'Operational + TF-IDF + PCA',\n",
    "    'operational+word2vec+pca': 'Operational + Word2Vec + PCA',\n",
    "    'operational+openai+pca': 'Operational + OpenAI + PCA'\n",
    "}\n",
    "\n",
    "with open('./data/results_final.json') as jsonfile:\n",
    "    result_only_embeddings = json.load(jsonfile)\n",
    "    \n",
    "result_only_embeddings = pd.DataFrame(result_only_embeddings)\n",
    "\n",
    "result_only_embeddings['model_name'] = result_only_embeddings['model_name'].map(model_name_mapping)\n",
    "result_only_embeddings['version'] = result_only_embeddings['version'].map(version_mapping)\n",
    "\n",
    "def plot_grouped_lines(result_only_features, metrics):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    models = result_only_features['model_name'].unique()\n",
    "    versions = result_only_features['version'].unique()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        for version in versions:\n",
    "            data = result_only_features[result_only_features['version'] == version].set_index('model_name')[metric].reindex(models)\n",
    "            axs[idx].plot(models, data, marker='o', label=version)\n",
    "        \n",
    "        axs[idx].set_title(f'{metric.upper()} Comparison')\n",
    "        axs[idx].set_xlabel(None)\n",
    "        axs[idx].set_ylabel(metric.upper())\n",
    "        axs[idx].set_xticks(np.arange(len(models)))\n",
    "        axs[idx].set_xticklabels(models, rotation=35, ha='right')\n",
    "        axs[idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(versions), bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig('./images/models_and_metrics.png', facecolor='white', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "metrics = ['rmse', 'mae', 'mape', 'r²']\n",
    "plot_grouped_lines(result_only_embeddings, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/results_final.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "data['r²'] = -data['r²']\n",
    "\n",
    "data['rmse_rank'] = data['rmse'].rank(ascending=True)\n",
    "data['mae_rank'] = data['mae'].rank(ascending=True)\n",
    "data['r2_rank'] = data['r²'].rank(ascending=True)\n",
    "data['mape_rank'] = data['mape'].rank(ascending=True)\n",
    "\n",
    "data['total_rank'] = data[['rmse_rank', 'mae_rank', 'r2_rank', 'mape_rank']].sum(axis=1)\n",
    "\n",
    "best_models = data.sort_values(by='total_rank').reset_index().head(10)\n",
    "\n",
    "best_models['r²'] = -best_models['r²']\n",
    "\n",
    "best_models[['model_name', 'version', 'rmse', 'mae', 'r²', 'mape', 'total_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', dtype=dtypes)[all_columns]\n",
    "val = pd.read_csv('./data/train.csv', dtype=dtypes)[all_columns]\n",
    "test = pd.read_csv('./data/test.csv', dtype=dtypes)[all_columns]\n",
    "\n",
    "train_features = preprocessing_subject(train)\n",
    "val_features = preprocessing_subject(val)\n",
    "test_features = preprocessing_subject(test)\n",
    "\n",
    "train_encoded, test_encoded = encoding(train_features, test_features)\n",
    "\n",
    "train_normalized, test_normalized = normalize(train_encoded, test_encoded)\n",
    "\n",
    "train_dropped = train_normalized.drop(columns=['SUBJECT'])\n",
    "test_dropped = test_normalized.drop(columns=['SUBJECT'])\n",
    "\n",
    "train_dropped.EMBEDDING_OPENAI = train_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)\n",
    "test_dropped.EMBEDDING_OPENAI = test_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = train_final(train_dropped, test_dropped, 'EMBEDDING_OPENAI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
