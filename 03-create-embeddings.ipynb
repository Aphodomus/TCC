{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import unicodedata\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "env_path = find_dotenv('keys.env')\n",
    "API_KEY = ''\n",
    "\n",
    "if not env_path:\n",
    "    print(\"O arquivo keys.env não foi encontrado. Certifique-se de que ele exista no diretório raiz do projeto.\")\n",
    "else:\n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "    if API_KEY is None:\n",
    "        print(\"A variável API_KEY não foi encontrada no arquivo keys.env.\")\n",
    "    else:\n",
    "        print(f\"API Key: {API_KEY}\")\n",
    "\n",
    "        def use_api(API_KEY):\n",
    "            print(f\"Usando a chave API: {API_KEY}\")\n",
    "\n",
    "        use_api(API_KEY)\n",
    "\n",
    "client = OpenAI(max_retries=5, api_key=API_KEY)\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "nlp = spacy.load('pt_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_preprocessed.csv')\n",
    "df['COMBINED_TEXT'] = df['SENDER']+ ' | ' + df['SUBJECT']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_acentos(text):\n",
    "    normalized_text = unicodedata.normalize('NFD', text)\n",
    "    text_without_accents = ''.join(c for c in normalized_text if unicodedata.category(c) != 'Mn')\n",
    "    return text_without_accents\n",
    "\n",
    "def remove_numeros(sentence):\n",
    "    return re.sub(r'\\b\\d+\\w*\\b', '', sentence)\n",
    "\n",
    "def preprocessing_text(text, language='portuguese'):\n",
    "    text = remove_acentos(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('r$', '')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = remove_numeros(text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized_tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df['SUBJECT_PREPROCESSED'] = df['COMBINED_TEXT'].apply(preprocessing_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text: str, model=\"text-embedding-3-small\", **kwargs) -> List[float]:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    response = client.embeddings.create(input=[text], model=model, **kwargs)\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def apply_tfidf(data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data)\n",
    "    return list(tfidf_matrix.toarray())\n",
    "\n",
    "def apply_word2vec(data):\n",
    "    sentences = [sentence.split() for sentence in data]\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=200,\n",
    "        window=2,\n",
    "        min_count=2,\n",
    "        negative=5,\n",
    "        workers=1,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_vecs = [model.wv[word] for word in sentence if word in model.wv]\n",
    "        if word_vecs:\n",
    "            sentence_embedding = np.mean(word_vecs, axis=0)\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(model.vector_size)\n",
    "        embeddings.append(sentence_embedding)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "df['EMBEDDING_TFIDF'] = apply_tfidf(df['SUBJECT_PREPROCESSED'])\n",
    "df['EMBEDDING_WORD2VEC'] = apply_word2vec(df['SUBJECT_PREPROCESSED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMBEDDING_OPENAI'] = df['COMBINED_TEXT'].apply(lambda x: get_embeddings(x, model=embedding_model))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMBEDDING_TFIDF'] = df['EMBEDDING_TFIDF'].apply(lambda x: x.tolist())\n",
    "df['EMBEDDING_WORD2VEC'] = df['EMBEDDING_WORD2VEC'].apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/data_preprocessed_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
