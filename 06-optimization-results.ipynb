{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import emoji\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nlp = spacy.load('pt_core_news_md')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [\n",
    "    'SUBJECT',\n",
    "    'SENDER',\n",
    "    'EMAIL',\n",
    "    'DELIVERED',\n",
    "    'HOUR_OF_DAY',\n",
    "    'WEEKDAY',\n",
    "    'TIME_OF_DAY',\n",
    "    'DELIVERED_SCALED',\n",
    "    'OPEN_RATE',\n",
    "    'HOUR_OPEN_RATE',\n",
    "    'WEEKDAY_OPEN_RATE',\n",
    "    'TIME_OF_DAY_OPEN_RATE',\n",
    "    'SENDER_OPEN_RATE',\n",
    "    'EMAIL_OPEN_RATE',\n",
    "    'COMBINED_TEXT',\n",
    "    'SUBJECT_PREPROCESSED',\n",
    "    'EMBEDDING_TFIDF',\n",
    "    'EMBEDDING_WORD2VEC',\n",
    "    'EMBEDDING_OPENAI'\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    'SUBJECT': 'string',\n",
    "    'SENDER': 'category',\n",
    "    'EMAIL': 'category',\n",
    "    'DELIVERED': 'int64',\n",
    "    'OPEN_RATE': 'float64',\n",
    "    'HOUR_OF_DAY': 'int64',\n",
    "    'WEEKDAY': 'category',\n",
    "    'TIME_OF_DAY': 'category',\n",
    "    'DELIVERED_SCALED': 'float64',\n",
    "    'OPEN_RATE': 'float64',\n",
    "    'HOUR_OPEN_RATE': 'float64',\n",
    "    'WEEKDAY_OPEN_RATE': 'float64',\n",
    "    'TIME_OF_DAY_OPEN_RATE': 'float64',\n",
    "    'SENDER_OPEN_RATE': 'float64',\n",
    "    'EMAIL_OPEN_RATE': 'float64',\n",
    "    'COMBINED_TEXT': 'string',\n",
    "    'SUBJECT_PREPROCESSED': 'string',\n",
    "    'EMBEDDING_TFIDF': 'string',\n",
    "    'EMBEDDING_WORD2VEC': 'string',\n",
    "    'EMBEDDING_OPENAI': 'string'\n",
    "}\n",
    "\n",
    "def contains_emoji(text):\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def extract_features_subject(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'OPEN_RATE',\n",
    "        'DELIVERED',\n",
    "        'DELIVERED_SCALED',\n",
    "        'HOUR_OF_DAY',\n",
    "        'WEEKDAY',\n",
    "        'TIME_OF_DAY',\n",
    "        'HOUR_OPEN_RATE',\n",
    "        'WEEKDAY_OPEN_RATE',\n",
    "        'TIME_OF_DAY_OPEN_RATE',\n",
    "        'SENDER_OPEN_RATE',\n",
    "        'EMAIL_OPEN_RATE',\n",
    "        'EMAIL',\n",
    "        'SUBJECT',\n",
    "        'EMBEDDING_TFIDF',\n",
    "        'EMBEDDING_WORD2VEC',\n",
    "        'EMBEDDING_OPENAI'\n",
    "    ]\n",
    "    \n",
    "    columns_to_drop = all_columns.copy()\n",
    "    for col in columns_to_keep:\n",
    "        columns_to_drop.remove(col)\n",
    "    \n",
    "    new_df['SUBJECT_LENGTH'] = new_df['SUBJECT'].apply(len)\n",
    "    new_df['SUBJECT_WORD_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(x.split()))\n",
    "    new_df['SUBJECT_SPECIAL_CHARS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "    new_df['SUBJECT_NUMBERS_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    new_df['SUBJECT_HAS_EMOJI'] = new_df['SUBJECT'].apply(contains_emoji)\n",
    "    \n",
    "    new_df['SUBJECT_UPPERCASE_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "    new_df['SUBJECT_NAMED_ENTITIES_COUNT'] = new_df['SUBJECT'].apply(lambda x: len(nlp(x).ents))\n",
    "\n",
    "    new_df['SUBJECT_LETTER_RATIO'] = new_df['SUBJECT'].apply(lambda x: sum(c.isalpha() for c in x) / len(x))\n",
    "    new_df['SUBJECT_VOWEL_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'aeiouáéíóúâêîôûãõ' for c in x))\n",
    "    new_df['SUBJECT_CONSONANT_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(c.lower() in 'bcçdfghjklmnpqrstvwxyz' for c in x))\n",
    "    new_df['SUBJECT_PUNCTUATION_COUNT'] = new_df['SUBJECT'].apply(lambda x: sum(el in string.punctuation for el in x))\n",
    "    \n",
    "    return new_df.drop(columns=columns_to_drop)\n",
    "\n",
    "def preprocessing_subject(data):\n",
    "    feature_selected = extract_features_subject(data)\n",
    "    \n",
    "    return feature_selected\n",
    "\n",
    "def encoding(train, val, test):\n",
    "    categorical_columns = train.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    encoded_train = train.copy()\n",
    "    encoded_val = val.copy()\n",
    "    encoded_test = test.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        encoder.fit(train[[col]])\n",
    "        \n",
    "        train_encoded = encoder.transform(train[[col]]).toarray()\n",
    "        val_encoded = encoder.transform(val[[col]]).toarray()\n",
    "        test_encoded = encoder.transform(test[[col]]).toarray()\n",
    "        \n",
    "        columns = encoder.get_feature_names_out([col])\n",
    "        \n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=columns, index=train.index)\n",
    "        val_encoded_df = pd.DataFrame(val_encoded, columns=columns, index=val.index)\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=columns, index=test.index)\n",
    "        \n",
    "        encoded_train = pd.concat([encoded_train.reset_index(drop=True), train_encoded_df.reset_index(drop=True)], axis=1)\n",
    "        encoded_val = pd.concat([encoded_val.reset_index(drop=True), val_encoded_df.reset_index(drop=True)], axis=1)\n",
    "        encoded_test = pd.concat([encoded_test.reset_index(drop=True), test_encoded_df.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        encoded_train = encoded_train.drop(col, axis=1)\n",
    "        encoded_val = encoded_val.drop(col, axis=1)\n",
    "        encoded_test = encoded_test.drop(col, axis=1)\n",
    "    \n",
    "    return encoded_train, encoded_val, encoded_test\n",
    "\n",
    "def normalize(train, val, test):\n",
    "    num_columns = train.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    columns_to_remove = ['OPEN_RATE'] + [col for col in num_columns if 'EMBEDDING' in col]\n",
    "    num_columns = num_columns.drop(columns_to_remove)\n",
    "    \n",
    "    X_train_num = train[num_columns]\n",
    "    X_val_num = val[num_columns]\n",
    "    X_test_num = test[num_columns]\n",
    "\n",
    "    train_non_num = train.drop(columns=num_columns)\n",
    "    val_non_num = val.drop(columns=num_columns)\n",
    "    test_non_num = test.drop(columns=num_columns)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_num)\n",
    "\n",
    "    X_train_normalized = scaler.transform(X_train_num)\n",
    "    X_val_normalized = scaler.transform(X_val_num)\n",
    "    X_test_normalized = scaler.transform(X_test_num)\n",
    "\n",
    "    train_normalized_num = pd.DataFrame(X_train_normalized, columns=num_columns, index=train.index)\n",
    "    val_normalized_num = pd.DataFrame(X_val_normalized, columns=num_columns, index=val.index)\n",
    "    test_normalized_num = pd.DataFrame(X_test_normalized, columns=num_columns, index=test.index)\n",
    "\n",
    "    train_final = pd.concat([train_normalized_num, train_non_num], axis=1)\n",
    "    val_final = pd.concat([val_normalized_num, val_non_num], axis=1)\n",
    "    test_final = pd.concat([test_normalized_num, test_non_num], axis=1)\n",
    "\n",
    "    return train_final, val_final, test_final\n",
    "\n",
    "def expand_embeddings(data, column_name):\n",
    "    if column_name in data.columns:\n",
    "        embeddings_df = pd.DataFrame(data[column_name].tolist())\n",
    "        embeddings_df.columns = [f'{column_name}_{i}' for i in range(embeddings_df.shape[1])]\n",
    "        return embeddings_df\n",
    "    else:\n",
    "        print(f\"A coluna {column_name} não foi encontrada.\")\n",
    "    return data\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, r2, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', dtype=dtypes)[all_columns]\n",
    "val = pd.read_csv('./data/val.csv', dtype=dtypes)[all_columns]\n",
    "test = pd.read_csv('./data/test.csv', dtype=dtypes)[all_columns]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = preprocessing_subject(train)\n",
    "val_features = preprocessing_subject(val)\n",
    "test_features = preprocessing_subject(test)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, val_encoded, test_encoded = encoding(train_features, val_features, test_features)\n",
    "train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normalized, val_normalized, test_normalized = normalize(train_encoded, val_encoded, test_encoded)\n",
    "train_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns and Float Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dropped = train_normalized.drop(columns=['SUBJECT', 'EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC'])\n",
    "val_dropped = val_normalized.drop(columns=['SUBJECT', 'EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC'])\n",
    "test_dropped = test_normalized.drop(columns=['SUBJECT', 'EMBEDDING_TFIDF', 'EMBEDDING_WORD2VEC'])\n",
    "\n",
    "train_dropped.EMBEDDING_OPENAI = train_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)\n",
    "val_dropped.EMBEDDING_OPENAI = val_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)\n",
    "test_dropped.EMBEDDING_OPENAI = test_normalized.EMBEDDING_OPENAI.apply(eval).apply(np.array)\n",
    "\n",
    "train_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = f'EMBEDDING_OPENAI'\n",
    "train_embeddings = expand_embeddings(train_dropped, column_name)\n",
    "val_embeddings = expand_embeddings(val_dropped, column_name)\n",
    "test_embeddings = expand_embeddings(test_dropped, column_name)\n",
    "\n",
    "pca = PCA(n_components=6, random_state=42)\n",
    "train_vecs_df = pca.fit_transform(train_embeddings)\n",
    "val_vecs_df = pca.transform(val_embeddings)\n",
    "test_vecs_df = pca.transform(test_embeddings)\n",
    "\n",
    "pca_column_names = [\"openai_pca_\" + str(i) for i in range(train_vecs_df.shape[1])]\n",
    "train_vecs_df = pd.DataFrame(train_vecs_df, columns=pca_column_names, index=train.index)\n",
    "val_vecs_df = pd.DataFrame(val_vecs_df, columns=pca_column_names, index=val.index)\n",
    "test_vecs_df = pd.DataFrame(test_vecs_df, columns=pca_column_names, index=test.index)\n",
    "\n",
    "train_normalized_embeddings = pd.concat([train_dropped.drop(columns=['EMBEDDING_OPENAI']), train_vecs_df], axis=1)\n",
    "val_normalized_embeddings = pd.concat([val_dropped.drop(columns=['EMBEDDING_OPENAI']), val_vecs_df], axis=1)\n",
    "test_normalized_embeddings = pd.concat([test_dropped.drop(columns=['EMBEDDING_OPENAI']), test_vecs_df], axis=1)\n",
    "    \n",
    "X_train = train_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "y_train = train_normalized_embeddings['OPEN_RATE']\n",
    "X_val = val_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "y_val = val_normalized_embeddings['OPEN_RATE']\n",
    "X_test = test_normalized_embeddings.drop('OPEN_RATE', axis=1)\n",
    "y_test = test_normalized_embeddings['OPEN_RATE']\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "print(f'y_val: {y_val.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform_target_regressor(model):\n",
    "    return TransformedTargetRegressor(\n",
    "        regressor=model,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    )\n",
    "\n",
    "def load_model(name, version):\n",
    "    models = {\n",
    "        'catboost_regressor': CatBoostRegressor\n",
    "    }\n",
    "    \n",
    "    if name not in models:\n",
    "        raise ValueError(\"Model name not supported\")\n",
    "    \n",
    "    model = joblib.load(f'./models/final_model.pkl')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model_with_grid_search(model, model_name, version, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    param_grid = {\n",
    "        'regressor__iterations': [1000, 2000, 3000, 4000, 5000, 6000],\n",
    "        'regressor__max_depth': [4, 5, 6, 7, 8],\n",
    "        'regressor__learning_rate': [0.01, 0.03, 0.1, 0.3]\n",
    "    }\n",
    "    \n",
    "    model_regressor = create_transform_target_regressor(model)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_regressor,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    best_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        regressor__eval_set=(X_val, y_val),\n",
    "        regressor__use_best_model=True,\n",
    "        regressor__verbose=False,\n",
    "        regressor__plot=False\n",
    "    )\n",
    "    \n",
    "    catboost_model = best_model.regressor_\n",
    "    feature_importances = catboost_model.get_feature_importance()\n",
    "    \n",
    "    joblib.dump(best_model, f'./models/final_model.pkl')\n",
    "    model_regressor = load_model(model_name, version)\n",
    "    \n",
    "    y_pred = model_regressor.predict(X_test)\n",
    "    \n",
    "    rmse, mae, r2, mape = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'version': version,\n",
    "        'rmse': round(rmse, 4),\n",
    "        'mae': round(mae, 4),\n",
    "        'r²': round(r2, 4),\n",
    "        'mape': round(mape, 4)\n",
    "    }, feature_importances, y_pred\n",
    "\n",
    "def models_final(version, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    model_dict = {\n",
    "        'catboost_regressor': CatBoostRegressor(\n",
    "            random_seed=42,\n",
    "            silent=True,\n",
    "            eval_metric='RMSE'\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    print(f'\\nVersion - {version}')\n",
    "    \n",
    "    results = []\n",
    "    feature_importances = ''\n",
    "    y_pred = ''\n",
    "    for model_name, model in model_dict.items():\n",
    "        result, feature_importances, y_pred = train_model_with_grid_search(model, model_name, version, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        results.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    return results, feature_importances, y_pred\n",
    "\n",
    "result, feature_importances, y_pred = models_final('embedding_final', X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_diagnostics(y_test, predictions):\n",
    "    errors = predictions - y_test\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(errors, kde=True, bins=30)\n",
    "    plt.title('Histogram of Residual Errors')\n",
    "    plt.xlabel('Residual Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(False)\n",
    "    plt.savefig('./images/residual_errors.png', facecolor='white', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(y_test, predictions, alpha=0.5)\n",
    "    plt.title('Actual vs. Predicted Values')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.grid(False)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.savefig('./images/actual_predicted.png', facecolor='white', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_model_diagnostics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize_column_quantile(data, quantile):\n",
    "    for i in range(2, 1000):\n",
    "        discretizer = KBinsDiscretizer(n_bins=i, encode='ordinal', strategy='quantile')\n",
    "        discretizer.fit_transform(data.to_numpy().reshape(-1, 1))\n",
    "        num_of_edges = len(discretizer.bin_edges_[0])\n",
    "        \n",
    "        if num_of_edges == quantile:\n",
    "            return discretizer\n",
    "\n",
    "discretizer_frequency = discretize_column_quantile(test['OPEN_RATE'], 6)\n",
    "discretizer_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels_2 = ['A', 'B', 'C', 'D', 'E']\n",
    "test['QUARTILE'] = pd.cut(test['OPEN_RATE'], bins=discretizer_frequency.bin_edges_[0], labels=bin_labels_2, include_lowest=True)\n",
    "test[['OPEN_RATE', 'QUARTILE']].sort_values(by='OPEN_RATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['QUARTILE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = test.groupby('QUARTILE').agg(\n",
    "    OPEN_RATE_MIN=('OPEN_RATE', 'min'),\n",
    "    OPEN_RATE_MAX=('OPEN_RATE', 'max')\n",
    ").reset_index()\n",
    "\n",
    "agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['PREDICTIONS'] = y_pred\n",
    "test[['OPEN_RATE', 'PREDICTIONS', 'QUARTILE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_bins(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = (mean_absolute_percentage_error(y_true, y_pred)) * 100\n",
    "    return rmse, mae, mape\n",
    "\n",
    "results = []\n",
    "\n",
    "for label in bin_labels_2:\n",
    "    subset = test[test['QUARTILE'] == label]\n",
    "    rmse, mae, mape = calculate_metrics_bins(subset['OPEN_RATE'], subset['PREDICTIONS'])\n",
    "    results.append({'QUARTILE': label, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape})\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "\n",
    "merged_df = pd.merge(agg_data, metrics_df, on='QUARTILE')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "n, bins, patches = ax.hist(test['OPEN_RATE'], bins=66, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "for i, (min_rate, max_rate) in enumerate(zip(merged_df['OPEN_RATE_MIN'], merged_df['OPEN_RATE_MAX'])):\n",
    "    if i != 0:\n",
    "        ax.axvline(min_rate, color='black', linestyle='--')\n",
    "\n",
    "    ax.text((min_rate + max_rate) / 2, max(n) * 1.1, \n",
    "            f\"{merged_df['QUARTILE'][i]}\\nRMSE: {merged_df['RMSE'][i]:.2f}\\nMAE: {merged_df['MAE'][i]:.2f}\\nMAPE: {merged_df['MAPE'][i]:.2f}\", \n",
    "            horizontalalignment='center', verticalalignment='center', bbox=dict(facecolor='#a1b9bd', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel('Open Rate')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Open Rate by Quartile with Metrics')\n",
    "\n",
    "ax.set_ylim(top=max(n) * 1.2)\n",
    "ax.set_xlim(left=test['OPEN_RATE'].min(), right=test['OPEN_RATE'].max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/distribution_quartile.png', facecolor='white', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
